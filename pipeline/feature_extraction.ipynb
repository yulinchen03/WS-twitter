{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Load the GloVe pre-trained embeddings"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "402cf4212e9dbce7"
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2025-06-19T18:10:54.408709900Z",
     "start_time": "2025-06-19T18:10:54.399061600Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def load_glove_model(glove_file):\n",
    "    \"\"\"\n",
    "    Loads the GloVe word vectors from a specified file into a dictionary.\n",
    "    \"\"\"\n",
    "    print(f\"Loading GloVe model from {glove_file}...\")\n",
    "    model = {}\n",
    "    \n",
    "    file_size = os.path.getsize(glove_file)\n",
    "    \n",
    "    with open(glove_file, 'r', encoding=\"utf-8\") as f:\n",
    "        with tqdm(total=file_size, unit='B', unit_scale=True, desc=f\"Loading {os.path.basename(glove_file)}\") as pbar:\n",
    "            for line in f:\n",
    "                split_line = line.split()\n",
    "                word = split_line[0]\n",
    "                # The rest of the line is the vector\n",
    "                embedding = np.array([float(val) for val in split_line[1:]])\n",
    "                model[word] = embedding\n",
    "                \n",
    "                pbar.update(len(line.encode('utf-8')))\n",
    "                \n",
    "    print(f\"Loading complete. Total of {len(model)} words loaded!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GloVe model from ../data/GloVe/glove.twitter.27B.50d.txt...\n"
     ]
    },
    {
     "data": {
      "text/plain": "Loading glove.twitter.27B.50d.txt:   0%|          | 0.00/511M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1bde39f200424d0abb2df10ef9a574d4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading complete. Total of 1193514 words loaded!\n"
     ]
    }
   ],
   "source": [
    "glove_path = \"../data/GloVe/glove.twitter.27B.50d.txt\"\n",
    "\n",
    "# Load the 50-dimensional Twitter GloVe model\n",
    "glove_twitter_model = load_glove_model(glove_path)\n",
    "embedding_dim = 50"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-19T18:11:09.095690Z",
     "start_time": "2025-06-19T18:10:54.410779700Z"
    }
   },
   "id": "34efdfef63796e45"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Load the preprocessed data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "42353da6ac447239"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from CSV files...\n",
      "Loaded 'train.csv' with 4152 rows.\n",
      "Loaded 'test.csv' with 1039 rows.\n",
      "\n",
      "Verifying the first 5 rows of the loaded training data:\n"
     ]
    },
    {
     "data": {
      "text/plain": "        user_id  occupation_code  \\\n5127  265383481                8   \n3607   22364420                5   \n1689   16797684                2   \n4942   14871013                3   \n4317   75687820                6   \n\n                                              category  \\\n5127             Process, Plant and Machine Operatives   \n3607                        Skilled Trades Occupations   \n1689                          Professional Occupations   \n4942  Associate Professional and Technical Occupations   \n4317     Caring, Leisure and Other Service Occupations   \n\n                                       aggregated_words  \n5127  [abandoned, abilities, able, able, able, able,...  \n3607  [abandoned, abiding, ability, ability, able, a...  \n1689  [ability, ability, able, able, absolutely, acc...  \n4942  [abandon, abilities, ability, ability, able, a...  \n4317  [ability, ability, ability, absolutely, abuse,...  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>user_id</th>\n      <th>occupation_code</th>\n      <th>category</th>\n      <th>aggregated_words</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>5127</th>\n      <td>265383481</td>\n      <td>8</td>\n      <td>Process, Plant and Machine Operatives</td>\n      <td>[abandoned, abilities, able, able, able, able,...</td>\n    </tr>\n    <tr>\n      <th>3607</th>\n      <td>22364420</td>\n      <td>5</td>\n      <td>Skilled Trades Occupations</td>\n      <td>[abandoned, abiding, ability, ability, able, a...</td>\n    </tr>\n    <tr>\n      <th>1689</th>\n      <td>16797684</td>\n      <td>2</td>\n      <td>Professional Occupations</td>\n      <td>[ability, ability, able, able, absolutely, acc...</td>\n    </tr>\n    <tr>\n      <th>4942</th>\n      <td>14871013</td>\n      <td>3</td>\n      <td>Associate Professional and Technical Occupations</td>\n      <td>[abandon, abilities, ability, ability, able, a...</td>\n    </tr>\n    <tr>\n      <th>4317</th>\n      <td>75687820</td>\n      <td>6</td>\n      <td>Caring, Leisure and Other Service Occupations</td>\n      <td>[ability, ability, ability, absolutely, abuse,...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading data from CSV files...\")\n",
    "\n",
    "train_path = '../data/processed/train.pkl'\n",
    "test_path = '../data/processed/test.pkl'\n",
    "\n",
    "# Load the training set\n",
    "train_df = pd.read_pickle(train_path)\n",
    "print(f\"Loaded 'train.csv' with {len(train_df)} rows.\")\n",
    "\n",
    "# Load the test set\n",
    "test_df = pd.read_pickle(test_path)\n",
    "print(f\"Loaded 'test.csv' with {len(test_df)} rows.\")\n",
    "\n",
    "# --- Verify one of the loaded DataFrames ---\n",
    "print(\"\\nVerifying the first 5 rows of the loaded training data:\")\n",
    "train_df.head()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-19T18:11:10.140295900Z",
     "start_time": "2025-06-19T18:11:09.089440800Z"
    }
   },
   "id": "617125874c34ba79"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Extract features using GloVe embeddings and save to output"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b44b9b2089f093b3"
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def tf_idf(df):\n",
    "    print(\"--- Starting TF-IDF Weighted Feature Extraction ---\")\n",
    "    \n",
    "    df_copy = df.copy()\n",
    "        \n",
    "    # Step 1: Convert word lists to single strings for the vectorizer\n",
    "    # The TfidfVectorizer expects documents as strings, not lists of words.\n",
    "    df_copy['aggregated_text'] = df_copy['aggregated_words'].str.join(' ')\n",
    "    \n",
    "    # Step 2: Fit TfidfVectorizer on your text documents\n",
    "    # IMPORTANT: In your real project, you MUST fit this on your TRAINING data only.\n",
    "    print(\"Fitting TfidfVectorizer...\")\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_vectorizer.fit(df_copy['aggregated_text'])\n",
    "    \n",
    "    # Create a dictionary mapping words to their IDF scores\n",
    "    word_idf_weights = dict(zip(tfidf_vectorizer.get_feature_names_out(), tfidf_vectorizer.idf_))\n",
    "    print(\"TfidfVectorizer fit complete.\")\n",
    "    \n",
    "    return word_idf_weights"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-19T18:11:10.146972900Z",
     "start_time": "2025-06-19T18:11:10.135611200Z"
    }
   },
   "id": "a2b92b92ac441abb"
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [],
   "source": [
    "def create_tfidf_weighted_vector(word_list, model, idf_weights, embed_dim=50):\n",
    "        \"\"\"Calculates the TF-IDF weighted mean vector for a list of words.\"\"\"\n",
    "        vectors = []\n",
    "        weights = []\n",
    "\n",
    "        for word in word_list:\n",
    "            if word in model and word in idf_weights:\n",
    "                vectors.append(model[word])\n",
    "                # The weight is the word's IDF score\n",
    "                weights.append(idf_weights[word])\n",
    "\n",
    "        if not vectors:\n",
    "            return np.zeros(embed_dim)\n",
    "\n",
    "        # Calculate the weighted average of the vectors\n",
    "        weighted_mean_vector = np.average(vectors, axis=0, weights=weights)\n",
    "        \n",
    "        return weighted_mean_vector"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-19T18:11:10.158620Z",
     "start_time": "2025-06-19T18:11:10.141813800Z"
    }
   },
   "id": "98b6178063c74d94"
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating TF-IDF weighted feature vectors...\n",
      "../data/extracted/train.pkl\n",
      "--- Starting TF-IDF Weighted Feature Extraction ---\n",
      "Fitting TfidfVectorizer...\n",
      "TfidfVectorizer fit complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Creating TF-IDF Vectors:   0%|          | 0/4152 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5d39f07dac054310b816c1dcebe44734"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving data using Pickle...\n",
      "Dataset with 4152 rows saved to ../data/extracted/train.pkl\n",
      "../data/extracted/test.pkl\n",
      "--- Starting TF-IDF Weighted Feature Extraction ---\n",
      "Fitting TfidfVectorizer...\n",
      "TfidfVectorizer fit complete.\n"
     ]
    },
    {
     "data": {
      "text/plain": "Creating TF-IDF Vectors:   0%|          | 0/1039 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ee72559ddc8241798b87d3daee690392"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Saving data using Pickle...\n",
      "Dataset with 1039 rows saved to ../data/extracted/test.pkl\n",
      "Processing complete.\n"
     ]
    }
   ],
   "source": [
    "train_path_pkl = '../data/extracted/train.pkl'\n",
    "test_path_pkl = '../data/extracted/test.pkl'\n",
    "\n",
    "print(\"\\nCreating TF-IDF weighted feature vectors...\")\n",
    "\n",
    "for df, save_path in zip([train_df, test_df], [train_path_pkl, test_path_pkl]):\n",
    "    print(save_path)\n",
    "    tqdm.pandas(desc=\"Creating TF-IDF Vectors\")\n",
    "    \n",
    "    word_idf_weights = tf_idf(df)\n",
    "\n",
    "    df['fv'] = df['aggregated_words'].progress_apply(\n",
    "        lambda words: create_tfidf_weighted_vector(words, glove_twitter_model, word_idf_weights, embedding_dim))\n",
    "        \n",
    "    print(\"\\nSaving data using Pickle...\")\n",
    "    df.to_pickle(save_path)\n",
    "    print(f\"Dataset with {len(df)} rows saved to {save_path}\")\n",
    "    \n",
    "print(\"Processing complete.\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2025-06-19T18:11:55.033781500Z",
     "start_time": "2025-06-19T18:11:10.158620Z"
    }
   },
   "id": "7d86a542eb7eae9f"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
